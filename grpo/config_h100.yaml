# Chess GRPO Training Configuration - Optimized for Single H100 80GB
# ===================================================================

# Model Configuration
model:
  name_or_path: "your-chess-model-4b"  # Replace with your 4B model path
  max_seq_length: 4096  # Prompt + completion total

# Unsloth Configuration
unsloth:
  # LoRA mode (set to false for full fine-tuning)
  use_lora: true

  # 16-bit LoRA recommended for H100 (plenty of VRAM)
  # Set to true for smaller GPUs
  load_in_4bit: false

  # FP8 training - 1.4x faster on H100, ~50% less model memory
  # Requires H100/H200/A100/RTX 40xx
  use_fp8: false  # Set true for maximum speed

  # Gradient checkpointing - essential for long sequences
  # "unsloth" mode is 30% faster than standard checkpointing
  use_gradient_checkpointing: true

  # vLLM memory utilization - Unsloth Standby handles the rest
  # With UNSLOTH_VLLM_STANDBY=1, you can set this high
  gpu_memory_utilization: 0.85

  # FP8 KV cache - must match model dtype (use_fp8)
  # FlashInfer doesn't support mixed BF16 queries + FP8 KV cache
  use_fp8_kv_cache: false

# LoRA Configuration
lora:
  r: 32  # Larger rank for better learning capacity
  alpha: 32
  dropout: 0.0  # 0 is optimized in Unsloth
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

# Stockfish Configuration
stockfish:
  path: "/usr/games/stockfish"
  depth: 20  # Depth 20 for full reward
  threads: 8  # H100 systems usually have good CPUs
  hash_mb: 512

# Reward Configuration
# Rebalanced to emphasize move quality over format compliance
reward:
  # Stockfish match rewards (increased to dominate reward signal)
  top1_reward: 1.5     # Matches Stockfish best move
  top3_reward: 1.0     # In top 3 moves
  top5_reward: 0.6     # In top 5 moves
  legal_reward: 0.1    # Legal but not in top 5
  illegal_penalty: -0.5
  no_move_penalty: -1.0

  # Format rewards (reduced to avoid masking move quality signal)
  format_bonus: 0.1      # Correct <think>...</think><uci_move>...</uci_move>
  format_penalty: -0.1   # Wrong format
  xml_bonus: 0.05        # Exactly one of each tag
  xml_penalty: -0.15     # Wrong tag count

# Data Configuration  
data:
  # Game positions
  games_dataset: "Lichess/standard-chess-games"
  min_elo: 800  # Include beginner games
  sample_rate: 0.3
  skip_first_moves: 4
  skip_last_moves: 2
  
  # Puzzle positions
  puzzles_dataset: "Lichess/chess-puzzles"
  min_puzzle_rating: 800
  max_puzzle_rating: 2500
  puzzle_ratio: 0.3  # 30% puzzles, 70% games
  
  # ELO weighting (includes lower levels)
  elo_weights:
    800: 0.08
    1000: 0.10
    1200: 0.12
    1400: 0.15
    1600: 0.18
    1800: 0.20
    2000: 0.18
    2200: 0.12
    2400: 0.08
  
  num_train_samples: 50000

# GRPO Training Configuration - Optimized for H100
training:
  # =========================================================================
  # Batch Size / Rollout Analysis for H100 80GB with LoRA
  # =========================================================================
  #
  # Memory Budget (LoRA, BF16 base model, 2K completion):
  #   Model weights (BF16):     ~8 GB (frozen, no optimizer states)
  #   LoRA adapters (BF16):     ~0.5 GB (r=32, 7 modules)
  #   AdamW states (FP32):      ~2 GB (only for LoRA params)
  #   Gradients (BF16):         ~0.5 GB (only for LoRA params)
  #   Activations (unsloth):    ~8-12 GB (2K seq, optimized checkpointing)
  #   vLLM KV cache (FP8):      ~6-8 GB
  #   GRPO logits (8 gens):     ~12-16 GB
  #   ─────────────────────────────────
  #   Total:                    ~40-50 GB (plenty of headroom on 80GB)
  #
  # Effective batch size = batch_size × grad_accum × num_generations
  #   = 1 × 4 × 8 = 32 samples per optimizer step
  #
  # =========================================================================

  # Batch settings - GRPO uses batch_size=1, gradient accumulation for stability
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4

  # GRPO rollouts - 8 generations for good advantage estimation
  # More = better variance reduction, but diminishing returns past 8
  num_generations: 8
  max_prompt_length: 1024   # Input tokens (FEN + legal moves + instructions)
  max_completion_length: 2048  # Output tokens (thinking + move)
  
  # Generation parameters
  temperature: 0.7

  # KL penalty - set to 0.0 per 2025 research (Open-Reasoner-Zero, DAPO, etc.)
  # With verifiable rewards like Stockfish, KL penalty is unnecessary and suppresses exploration
  beta: 0.0

  # Reward scaling - "batch" is more robust than per-group normalization
  # Per Understanding R1-Zero-Like Training paper
  scale_rewards: "batch"

  # Ignore truncated outputs in loss computation
  mask_truncated_completions: true

  # DAPO Loss Configuration (2025 best practice for reasoning tasks)
  # loss_type options: "grpo", "dapo", "sapo", "dr_grpo", "bnpo"
  # DAPO achieved 50% on AIME 2024 vs 30% baseline
  loss_type: "dapo"

  # Asymmetric clipping (DAPO Clip-Higher strategy)
  # epsilon: lower bound for suppression (standard PPO/GRPO value)
  # epsilon_high: upper bound for encouragement (allows more exploration for low-prob tokens)
  epsilon: 0.2
  epsilon_high: 0.28

  # Optimizer - adamw_torch_fused is fastest on H100
  # Alternatives: paged_adamw_8bit (memory efficient), adamw_torch (standard)
  # Note: DAPO paper uses learning_rate: 1.0e-6, but 5e-6 works well with LoRA
  learning_rate: 5.0e-6
  weight_decay: 0.1
  warmup_ratio: 0.1
  lr_scheduler_type: cosine
  optim: adamw_torch_fused
  
  # Training duration
  # For GRPO, expect ~300+ steps to see reward improvement
  # 12+ hours training recommended for good results
  num_train_epochs: 1
  max_steps: -1  # Use num_train_epochs
  
  # Gradient clipping
  max_grad_norm: 0.1
  
  # Logging - log every step to monitor reward
  logging_steps: 1
  save_steps: 500
  
  # vLLM integration (Unsloth handles automatically)
  use_vllm: true
  
  # Output
  output_dir: "./chess_grpo_h100_output"

# Weights & Biases Configuration
wandb:
  enabled: true
  project: "chess-grpo"
  run_name: null  # Auto-generated if null
  entity: null    # Your wandb team/user (optional)

# =========================================================================
# H100 Performance Notes (LoRA)
# =========================================================================
#
# Throughput expectations:
# - vLLM generation: ~3000-5000 tokens/s
# - Training step (8 generations, 2K completion): ~15-30 seconds
# - 50K samples × 1 epoch ≈ 8-15 hours
#
# =========================================================================
# Troubleshooting OOM (unlikely with LoRA on H100)
# =========================================================================
#
# Option 1: Reduce num_generations
#   num_generations: 4    # Minimum viable for advantage estimation
#
# Option 2: Enable FP8 model weights
#   use_fp8: true         # Saves ~4GB, 1.4x faster
#
# Option 3: Enable 4-bit quantization
#   load_in_4bit: true    # Saves ~4GB, slight quality tradeoff
#
# Option 4: Lower vLLM memory
#   gpu_memory_utilization: 0.7
