# Chess GRPO Training Configuration - Full Fine-Tuning (No LoRA)
# ==================================================================
# Optimized for Single H100 80GB GPU
#
# WARNING: Full fine-tuning requires significantly more VRAM than LoRA.
# This config is tuned for 4B models. For larger models, consider:
# - Multi-GPU with DeepSpeed ZeRO-3 or FSDP
# - Using LoRA instead (config_h100.yaml)

# Model Configuration
model:
  name_or_path: "your-chess-model-4b"  # Replace with your model path
  max_seq_length: 4096  # Prompt + completion total

# Unsloth Configuration
unsloth:
  # Full fine-tuning mode (no LoRA)
  use_lora: false

  # 16-bit base weights
  load_in_4bit: false

  # FP8 training - REQUIRED for full fine-tuning with 2K completion
  # Saves ~4GB model memory, critical for fitting in 80GB
  use_fp8: true

  # Gradient checkpointing - REQUIRED for full fine-tuning
  use_gradient_checkpointing: true

  # vLLM memory utilization - lower for full fine-tuning
  gpu_memory_utilization: 0.6

  # FP8 KV cache - 2x less KV cache memory
  use_fp8_kv_cache: true

# Stockfish Configuration
stockfish:
  path: "/usr/games/stockfish"
  depth: 20
  threads: 8
  hash_mb: 512

# Reward Configuration
# Rebalanced to emphasize move quality over format compliance
reward:
  # Stockfish match rewards (increased to dominate reward signal)
  top1_reward: 1.5     # Matches Stockfish best move
  top3_reward: 1.0     # In top 3 moves
  top5_reward: 0.6     # In top 5 moves
  legal_reward: 0.1    # Legal but not in top 5
  illegal_penalty: -0.5
  no_move_penalty: -1.0

  # Format rewards (reduced to avoid masking move quality signal)
  format_bonus: 0.1
  format_penalty: -0.1
  xml_bonus: 0.05
  xml_penalty: -0.15

# Data Configuration
data:
  # Game positions
  games_dataset: "Lichess/standard-chess-games"
  min_elo: 800
  sample_rate: 0.3
  skip_first_moves: 4
  skip_last_moves: 2

  # Puzzle positions
  puzzles_dataset: "Lichess/chess-puzzles"
  min_puzzle_rating: 800
  max_puzzle_rating: 2500
  puzzle_ratio: 0.3

  # ELO weighting
  elo_weights:
    800: 0.08
    1000: 0.10
    1200: 0.12
    1400: 0.15
    1600: 0.18
    1800: 0.20
    2000: 0.18
    2200: 0.12
    2400: 0.08

  num_train_samples: 50000

# GRPO Training Configuration - Full Fine-Tuning
training:
  # =========================================================================
  # Batch Size / Rollout Analysis for H100 80GB - Full Fine-Tuning
  # =========================================================================
  #
  # Memory Budget (FP8 model + FP32 optimizer, 2K completion):
  #   Model weights (FP8):      ~4 GB
  #   AdamW states (FP32):      ~32 GB (2 moments × 4B params × 4 bytes)
  #   Gradients (BF16):         ~8 GB
  #   Activations (checkpt):    ~12-16 GB (2K sequence)
  #   GRPO logits storage:      ~4 GB per generation
  #   vLLM KV cache (FP8):      ~6-8 GB
  #   ─────────────────────────────────
  #   With 2 generations:       ~70-76 GB ✓
  #   With 4 generations:       ~78-84 GB ✗ (OOM risk)
  #
  # Effective batch size = batch_size × grad_accum × num_generations
  #   = 1 × 16 × 2 = 32 samples per optimizer step
  #
  # =========================================================================

  # Batch settings
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16   # Higher to compensate for 2 generations

  # GRPO rollouts - 2 is minimum viable for full fine-tuning with 2K completion
  # Gradient accumulation compensates for fewer generations
  num_generations: 2
  max_prompt_length: 512
  max_completion_length: 2048       # Full reasoning trace

  # Generation parameters
  temperature: 0.7

  # KL penalty - 0.0 per 2025 research (verifiable rewards don't need KL)
  beta: 0.0

  # Reward scaling - batch-level is more robust than per-prompt
  scale_rewards: "batch"

  # Ignore truncated outputs in loss
  mask_truncated_completions: true

  # DAPO Loss Configuration (2025 best practice)
  loss_type: "dapo"
  epsilon: 0.2
  epsilon_high: 0.28

  # =========================================================================
  # Optimizer - Full Fine-Tuning Settings
  # =========================================================================
  # adamw_torch_fused: PyTorch's fused CUDA kernel, ~20% faster than standard
  # Requires PyTorch 2.0+ with CUDA. Falls back to adamw_torch if unavailable.
  #
  # Learning rate: 5x lower than LoRA since we're updating all parameters
  # Weight decay: Lower to prevent over-regularization of base weights
  # =========================================================================
  optim: adamw_torch_fused
  learning_rate: 1.0e-6
  weight_decay: 0.01
  warmup_ratio: 0.1
  lr_scheduler_type: cosine

  # Training duration
  num_train_epochs: 1
  max_steps: -1

  # Gradient clipping - slightly higher for full fine-tuning stability
  max_grad_norm: 0.5

  # Logging
  logging_steps: 1
  save_steps: 250  # More frequent saves (full checkpoints are ~16GB)

  # vLLM integration
  use_vllm: true

  # Output
  output_dir: "./chess_grpo_h100_full_output"

# Weights & Biases Configuration
wandb:
  enabled: true
  project: "chess-grpo"
  run_name: null  # Auto-generated if null
  entity: null    # Your wandb team/user (optional)

# =========================================================================
# Memory Budget (4B Model - Full Fine-Tuning, 2K Completion)
# =========================================================================
#
# Default config (FP8 model, 2 generations, 2K completion):
# ─────────────────────────────────────────────────────────────────────────
# Component                          | Memory    | Notes
# -----------------------------------|-----------|---------------------------
# Model weights (FP8)                | ~4 GB     | 4B × 1 byte
# AdamW optimizer states (FP32)      | ~32 GB    | 4B × 4 bytes × 2 moments
# Gradients (BF16)                   | ~8 GB     | 4B × 2 bytes
# Activations (gradient checkpt)     | ~12-16 GB | 2K sequence length
# vLLM KV cache (FP8, 2K ctx)        | ~6-8 GB   |
# GRPO logits (2 generations)        | ~8 GB     |
# Buffer/fragmentation               | ~4 GB     |
# ─────────────────────────────────────────────────────────────────────────
# Total                              | ~74-80 GB | Fits on H100 80GB
#
# =========================================================================
# Throughput Expectations
# =========================================================================
# - Full fine-tuning is ~2-3x slower than LoRA per step
# - vLLM generation: ~2000-4000 tokens/s
# - Training step (2 generations, 2K completion): ~30-60 seconds
# - 50K samples × 1 epoch ≈ 20-40 hours
#
# =========================================================================
# Troubleshooting OOM
# =========================================================================
#
# Option 1: Lower GPU utilization for vLLM
#   gpu_memory_utilization: 0.5
#
# Option 2: Reduce completion length
#   max_completion_length: 1024   # Significant memory savings
#
# Option 3: Use 4-bit quantization (quality tradeoff)
#   load_in_4bit: true
#
# Option 4: Switch to LoRA (recommended)
#   Use config_h100.yaml instead - 3x faster, much more memory headroom
